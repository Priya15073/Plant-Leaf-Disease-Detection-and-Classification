# -*- coding: utf-8 -*-
"""Xception_final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1HPdUUC1lfaeGcydBV5b3OXuUUJH5gdeI
"""

#Priya Rajpurohit 2015073
#Sakshi Saini 2017092

import keras,os
from keras.models import Sequential
from keras.layers import Dense, Conv2D, MaxPool2D , Flatten
from keras.preprocessing.image import ImageDataGenerator
import numpy as np

from keras.applications.xception import Xception
from keras.preprocessing import image
from keras.applications.xception import preprocess_input, decode_predictions
import numpy as np

import tensorflow_datasets as tfds
import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt

import keras
from keras.layers.core import Layer
import keras.backend as K

from keras.models import Model
from keras.layers import Conv2D, MaxPool2D,  \
    Dropout, Dense, Input, concatenate,      \
    GlobalAveragePooling2D, AveragePooling2D,\
    Flatten

import cv2 
import numpy as np 
from keras import backend as K 
from keras.utils import np_utils

import math 
from keras.optimizers import SGD 
from keras.callbacks import LearningRateScheduler
from sklearn.model_selection import train_test_split

# ds = tfds.load("plant_village", split=tfds.Split.TRAIN, batch_size=-1)
# ds = tfds.as_numpy(ds)

# images, labels = ds["image"], ds["label"]

from google.colab import drive
drive.mount('/content/drive')

!unzip -uq "/content/drive/My Drive/Segmented.zip"

import os

path = '/content/Segmented/'
images=[]
labels=[]
# r=root, d=directories, f = files
lable = 1
labels = []
for r, d, f in os.walk(path):
    for folder in d:
        
        for r1, d1, f1 in os.walk(os.path.join(r, folder)):
          for file in f1:
            # print(file)
            images.append( cv2.imread(os.path.join(r1, file) ))
            labels.append(lable)

          lable = lable +1
images=np.array(images)
images.shape

x_train, x_test, y_train, y_test = train_test_split( images[:10000], labels[:10000], test_size=0.3, random_state=42, shuffle=True )
print(x_train.shape, x_test.shape)

from keras.models import Model

image_input = Input(shape=(224,224, 3))
 
res=Xception(input_tensor=image_input,weights='imagenet', include_top=False)
res.layers.pop()
for layer in res.layers:
  layer.trainable = False
res = Model(inputs=res.inputs, outputs=res.layers[-1].output)
model = Sequential()
model.add(res)
model.add(GlobalAveragePooling2D(name='avg_pool') )  
model.add(Dense(256, activation='relu', input_shape=res.output_shape[1:]))
model.add(Dropout(0.5))

model.add(Dense(128, activation='relu'))
model.add(Dropout(0.5))    

model.add(Dense(38, activation='softmax'))


# x = GlobalAveragePooling2D()(res.output)
# x=Dropout(0.3)(x)
# x=Dense(1024,activation='relu')(x) #dense layer 2
# x=Dense(512,activation='relu')(x)


# prediction = Dense(38, activation='softmax')(x)

model.summary()

# # X_train = np.array([cv2.resize(img, (img_rows,img_cols)) for img in X_train[:,:,:,:]])
# # X_valid = np.array([cv2.resize(img, (img_rows,img_cols)) for img in X_valid[:,:,:,:]])


#images, labels = ds["image"], ds["label"]
n1 = 224
m1 = 224


features = []
for image in x_train:

  image = cv2.resize(image , (n1,m1) )

  image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))
  image = preprocess_input(image)

  feature = model.predict(image)
  
  features.append(feature)

featurestest = []
for image in x_test:

  image = cv2.resize(image , (n1,m1) )

  image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))
  image = preprocess_input(image)

  feature = model.predict(image)
  
  featurestest.append(feature)

resized = []
n = 224
m = 224

for image in x_train:
  image = cv2.resize( image, (n, m) )
  resized.append(image)
resized_t = []
n = 224
m = 224

for image in x_test:
  image = cv2.resize( image, (n, m) )
  resized_t.append(image)

features = np.array(features)
featurestest=np.array(featurestest)

y_train1 = np_utils.to_categorical(y_train, 38)
y_test1 = np_utils.to_categorical(y_test, 38)

print(featurestest.shape)
print(features.shape)
y_train1.shape

resized=np.array(resized)
resized_t=np.array(resized_t)

model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])
history1=model.fit(resized,y_train1, validation_data=(resized_t,y_test1),batch_size=100,epochs=10,shuffle=True)

!mkdir -p saved_model
#tpu_model.save('saved_model/AlexNet') 
model.save('saved_model/Xception')

final_accuracy = history1.history["val_accuracy"][-5:]
print("FINAL ACCURACY MEAN-5: ", np.mean(final_accuracy))
model_json = model.to_json()
with open("model.json", "w") as json_file:
    json_file.write(model_json)
# serialize weights to HDF5
model.save_weights("model.h5")

def display_training_curves(training, validation, title, subplot):
  ax = plt.subplot(subplot)
  ax.plot(training)
  ax.plot(validation)
  ax.set_title('model '+ title)
  ax.set_ylabel(title)
  ax.set_xlabel('epoch')
  ax.legend(['training', 'validation'])

plt.subplots(figsize=(10,10))
plt.tight_layout()
display_training_curves(history1.history['accuracy'], history1.history['val_accuracy'], 'accuracy', 211)
display_training_curves(history1.history['loss'], history1.history['val_loss'], 'loss', 212)

n1=224
m1=224
features1 = []
for image in x_train:

  image = cv2.resize(image , (n1,m1) )

  image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))
  image = preprocess_input(image)

  feature = res.predict(image)
  
  features1.append(feature)

featurestest1 = []
for image in x_test:

  image = cv2.resize(image , (n1,m1) )

  image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))
  image = preprocess_input(image)

  feature = res.predict(image)
  
  featurestest1.append(feature)
features = np.array(features1)
featurestest=np.array(featurestest1)

features.shape

from sklearn.ensemble import RandomForestClassifier

features = features.reshape(7000,100352)

clf = RandomForestClassifier()
clf.fit(features,  y_train1)
featurestest = featurestest.reshape(3000,100352)

accuracy = clf.score(featurestest,y_test1)
accuracy *= 100
print(accuracy)
import h5py


h5_data    = 'segmented_xception.h5'
h5_labels  = 'labels_xception_seg.h5'

h5f_data = h5py.File(h5_data, 'w')
h5f_data.create_dataset('dataset_1', data=np.array(features))
h5f_data.close()
h5f_label = h5py.File(h5_labels, 'w')
h5f_label.create_dataset('dataset_1', data=np.array(y_train1))
h5f_label.close()
import pickle


filename = 'xception_rf_model_seg.sav'
pickle.dump(clf, open(filename, 'wb'))

from sklearn import svm

#features = features.reshape(16000,38)

clf1 = svm.SVC()
clf1.fit(features,  y_train)
#featurestest = featurestest.reshape(4000, 38)

accuracy = clf1.score(featurestest,y_test)
accuracy *= 100
print(accuracy)
import h5py





filename = 'xception_svm_model_seg.sav'
pickle.dump(clf1, open(filename, 'wb'))

!cp xception_svm_model_seg.sav "/content/drive/My Drive/Xception_seg_low"
!cp xception_rf_model_seg.sav "/content/drive/My Drive/Xception_seg_low"
!cp segmented_xception.h5 "/content/drive/My Drive/Xception_seg_low"
!cp labels_xception_seg.h5 "/content/drive/My Drive/Xception_seg_low"
!cp model.h5 "/content/drive/My Drive/Xception_seg_low"
!cp model.json "/content/drive/My Drive/Xception_seg_low"
!cp -r saved_model "/content/drive/My Drive/Xception_seg_low"

# !pip3 install elm
# !pip3 install --upgrade numpy folium imgaug

# import elm
# elmk=elm.ELMKernel()
# elmk.search_param(data, cv="kfold", of="accuracy", eval=10)

# tr_result = elmk.train(features,y_train)
# te_result = elmk.test(featurestest,y_test)

# print(te_result.get_accuracy)